**DeepSeek-V2**

* **Introduction:** DeepSeek-V2 is a 236 billion parameter Mixture-of-Experts (MoE) language model, with 21 billion parameters active per token. It is designed for efficiency and high performance in various language tasks.  
* **Architecture:**  
  * **Mixture-of-Experts (MoE):** Employs a MoE architecture for efficient scaling.  
  * **Multi-head Latent Attention (MLA):** Introduces a novel attention mechanism called Multi-head Latent Attention to enhance efficiency.  
  * **Grouped Query Attention (GQA):** Incorporates GQA for improved inference speed.  
* **Training:**  
  * **Pre-training Data:** Trained on 8.1 trillion tokens of diverse data.  
  * **Efficient Training:** Emphasizes training efficiency, achieving competitive performance with relatively lower training costs.  
* **Performance:** DeepSeek-V2 demonstrates strong performance across various benchmarks, outperforming models of similar and larger sizes in many evaluations. It is noted for its exceptional performance-to-cost ratio.  
* **Timeline:**  
  * **November 2023:** DeepSeek-V2 was released.  
* **Hugging Face Links:**  
  * [DeepSeek-V2 Hugging Face Page](https://www.google.com/url?sa=E&source=gmail&q=https://huggingface.co/deepseek-ai/deepseek-v2)  
  * [DeepSeek V2 (All Versions) \- a unsloth Collection \- Hugging Face](https://www.google.com/search?q=https://huggingface.co/collections/unsloth/deepseek-v2-all-versions-655a828a5c9c5a15b7928358)  
  * [DeepSeek-V2-Base Hugging Face Page](https://www.google.com/search?q=https://huggingface.co/deepseek-ai/DeepSeek-V2-Base)  
  * [TheBloke/DeepSeek-V2-Base-GGUF \- Hugging Face](https://www.google.com/search?q=https://huggingface.co/TheBloke/DeepSeek-V2-Base-GGUF) \- GGUF format for local use.  
  * [DeepSeek-V2 at main \- Hugging Face Files Tree](https://www.google.com/search?q=https://huggingface.co/deepseek-ai/deepseek-v2/tree/main)

**DeepSeekCoder-V2**

* **Introduction:** DeepSeekCoder-V2 is the second generation of the DeepSeek Coder models, designed for code-related tasks. It is built upon the DeepSeek-V2 architecture and further optimized for coding performance.  
* **Model Variants:**  
  * **DeepSeek Coder V2 Base:** The base model for code generation and related tasks.  
  * **DeepSeek Coder V2 Instruct:** Instruction-tuned version, optimized for instruction following in coding contexts.  
* **Architecture:** Inherits the efficient MoE architecture and MLA from DeepSeek-V2.  
* **Training Data:** Trained on 2 trillion tokens of code and code-related data.  
* **Performance:** DeepSeek Coder V2 models achieve state-of-the-art performance on code benchmarks, outperforming previous DeepSeek Coder models and other open-source coding models. The Instruct version excels in instruction following for coding tasks.  
* **Timeline:**  
  * **December 2023:** DeepSeekCoder-V2 models (Base and Instruct) were released.  
* **Hugging Face Links:**  
  * [DeepSeek Coder V2 Hugging Face Page](https://www.google.com/search?q=https://huggingface.co/deepseek-ai/deepseek-coder-v2)  
  * [DeepSeek Coder V2 Instruct Hugging Face Page](https://www.google.com/url?sa=E&source=gmail&q=https://huggingface.co/deepseek-ai/deepseek-coder-v2-instruct)  
  * [Deepseek Coder V2 (All Versions) \- a unsloth Collection \- Hugging Face](https://www.google.com/search?q=https://huggingface.co/collections/unsloth/deepseek-coder-v2-all-versions-65787c88e97c259c6211697c)  
  * [TheBloke/DeepSeekCoder-V2-Base-GGUF \- Hugging Face](https://www.google.com/search?q=https://huggingface.co/TheBloke/DeepSeekCoder-V2-Base-GGUF) \- GGUF format.  
  * [DeepSeek Coder V2 Instruct at main \- Hugging Face Files Tree](https://www.google.com/search?q=https://huggingface.co/deepseek-ai/deepseek-coder-v2-instruct/tree/main)

**DeepSeek-Math**

* **Introduction:** DeepSeek-Math is a series of models specialized in mathematical problem-solving. It is designed to tackle complex mathematical problems, particularly in competition-level mathematics.  
* **Model Versions:**  
  * **DeepSeekMath 7B:** A 7 billion parameter model.  
  * **DeepSeekMath 7B-Instruct:** Instruction-tuned version of the 7B model.  
* **Training Data:** Trained on a dataset of mathematical problems, including competition-level problems.  
* **Methodology:** Employs techniques to enhance mathematical reasoning and problem-solving abilities.  
* **Performance:** DeepSeekMath models achieve strong performance on mathematical benchmarks, demonstrating capabilities in solving challenging math problems. The Instruct version is designed for better interaction and instruction following in mathematical contexts.  
* **Timeline:**  
  * **November 2023:** DeepSeekMath 7B and 7B-Instruct were released.  
* **Hugging Face Links:**  
  * [DeepSeekMath 7B Hugging Face Page](https://www.google.com/search?q=https://huggingface.co/deepseek-ai/deepseek-math-7b)  
  * [DeepSeekMath 7B Instruct Hugging Face Page](https://www.google.com/url?sa=E&source=gmail&q=https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct)  
  * [Deepseek Math (All Versions) \- a unsloth Collection \- Hugging Face](https://www.google.com/search?q=https://huggingface.co/collections/unsloth/deepseek-math-all-versions-656330755c9c5a15b7929358)  
  * [TheBloke/DeepSeekMath-7B-Instruct-GGUF \- Hugging Face](https://www.google.com/search?q=https://huggingface.co/TheBloke/DeepSeekMath-7B-Instruct-GGUF) \- GGUF format.  
  * [DeepSeekMath 7B Instruct at main \- Hugging Face Files Tree](https://www.google.com/url?sa=E&source=gmail&q=https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct/tree/main)

**ESFT**

* **Introduction:** ESFT likely refers to "Efficient Supervised Fine-Tuning". While "ESFT" itself is not listed as a specific model on Hugging Face under "deepseek-ai", it is a methodology or technique used in training some DeepSeek models.  
* **Concept:** Efficient Supervised Fine-Tuning (ESFT) aims to optimize the supervised fine-tuning process to improve model performance and efficiency. This could involve techniques like data curation, efficient training algorithms, or specific hyperparameter settings.  
* **Context within DeepSeek:** The term ESFT is mentioned in the context of DeepSeek-R1, where it is used as one of the stages in its multi-stage training pipeline. DeepSeek-R1 incorporates both Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL).  
* **Timeline:** As ESFT is a technique, not a specific model, there isn't a release timeline in the same way as models. However, its application is evident in models released from late 2024 and early 2025, such as DeepSeek-R1.  
* **Hugging Face Links:**  
  * [DeepSeek-R1 Hugging Face Page](https://huggingface.co/deepseek-ai/DeepSeek-R1) \- Mentions SFT as part of the training process.  
  * [Release DeepSeek-R1 Â· deepseek-ai/DeepSeek-R1 at 5a56bdb \- Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-R1/commit/5a56bdbde75a16bdfbf3a8e9c852be3dfcfb8eef) \- Details the SFT stages in DeepSeek-R1 training.

I will continue researching the rest of the terms in the next response.